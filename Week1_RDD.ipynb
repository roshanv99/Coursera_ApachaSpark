{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "Welcome to exercise one of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll apply the basics of functional and parallel programming. \n\nLet\u2019s start with a simple example. Let\u2019s consider you have a list of integers.\n\nLet\u2019s find out what the size of this list is.\n\nNote that we already provide an RDD object, so please have a look at the RDD API in order to find out what function to use:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n\nThe following link contains additional documentation:\nhttps://spark.apache.org/docs/latest/rdd-programming-guide.html\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd = sc.parallelize(range(100))",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#lambda stands for anonymoys function. Means you do not need a function body.\n#Map is used to send the function to the data. This function allows us to pass a function as a parameter to RDD\nrdd.map(lambda x: x+1).take(10)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You should see \"100\" as answer. Now we want to know the sum of all elements. Please again, have a look at the API documentation and complete the code below in order to get the sum."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#We want to sum up all elements in the list ie take the Gussian Sum\n#We use the reduce function to roll up the list from left to right\n#Var a contains intermediate sum and var b stands for next elemet in the list, which takes the sum of everthing\n\nsc.parallelize(range(0,100)).reduce(lambda a,b:a+b)",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 18,
                    "data": {
                        "text/plain": "4950"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You should get \"4950\" as answer."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sum = 0\nfor i in range(0,100):\n    sum +=i\nsum",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 19,
                    "data": {
                        "text/plain": "4950"
                    },
                    "metadata": {}
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}