{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "Welcome to exercise three of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll create a DataFrame, register a temporary query table and issue SQL commands against it. \n\nLet\u2019s create a little data frame:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql import Row\n\ndf = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])\n\n# let's have a look what's inside\ndf.show()\n\n# let's print the schema\ndf.printSchema()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we register this DataFrame as query table and issue an SQL statement against it. Please note that the result of the SQL execution returns a new DataFrame we can work with."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# register dataframe as query table\ndf.createOrReplaceTempView('df_view')\n\n# execute SQL query\ndf_result = spark.sql('select value from df_view where id=2')\n\n#\u00a0examine contents of result\ndf_result.show()\n\n# get result as string\ndf_result.first().value"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Although we\u2019ll learn more about DataFrames next week, please try to find a way to count the rows in this DataFrame by looking at the API documentation. No worries, we\u2019ll cover DataFrames in more detail next week.\n\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.$$$"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}